# Elasticsearch

### 什么是倒排索引？

> 所谓的倒排索引，就是把你的数据内容先分词，每句话分成一个一个的关键词，然后记录好每个关键词对应出现在了哪些id标识的数据里。

### 分布式搜索引擎

> 把大量的索引数据拆散成多块，每台机器放一部分，然后利用多台机器对分散之后的数据进行搜索，所有操作全部是分布在多台机器上进行，形成了完整的分布式的架构。

## elasticsearch的核心概念

> 1. Near Realtime（NRT）：近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级；
> 2. Cluster：集群：包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称，默认是elasticsearch）来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常。
> 3. **Node：节点：**集群中的一个节点，节点也有一个名称（默认是随机分配的）。
> 4. **Index：索引：**包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类类似的或者相同的document。
> 5. **Type：类型**：每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field。
> 6. **Document&field：**文档：es中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。一个document里面有多个field，每个field就是一个数据字段。
> 7. **shard：分区**：单台机器无法存储大量数据，es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储。有了shard就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。
> 8. **replica：副本**：任何一个服务器随时可能故障或宕机，此时shard可能就会丢失，因此可以为每个shard创建多个replica副本。replica可以在shard故障时提供备用服务，保证数据不丢失，多个replica还可以提升搜索操作的吞吐量和性能。

### Shard数据分片机制

> index存储的数据量不要太大，因为控制单台机器上这个index的数据量，可以保证他的搜索性能更高。
>
> 所以这里就引入了一个概念：**Shard数据分片结构**。每个index你都可以指定创建多少个shard，每个shard就是一个数据分片，会负责存储这个index的一部分数据。
>
> 可以分布式的并行对一部分数据进行搜索，起到一个分布式搜索的效果，大幅度提升海量数据的搜索性能和吞吐量。
>
> **官方文档**
>
> Elasticsearch 中的数据组织成索引。每一个索引由一个或多个分片组成。每个分片是 Luncene 索引的一个实例，你可以把实例理解成自管理的搜索引擎，用于在 Elasticsearch 集群中对一部分数据进行索引和处理查询。
>
> 构建 Elasticsearch 集群的初期如果集群分片设置不合理，可能在项目的中后期就会出现性能问题。
>
> 通过分片，ES 把数据放在不同节点上，这样可以存储超过单节点容量的数据。而副本分片数的增加可以提高搜索的吞吐量 (主分片与副本都能处理查询请求，ES 会自动对搜索请求进行负载均衡)。

### **Replica多副本数据冗余机制**

> 为了实现高可用使用Replica多副本数据冗余机制。
>
> 在Elasticsearch里，就是支持对每个index设置一个replica数量的，也就是每个shard对应的replica副本的数量。
>
> 在上述的replica机制下，每个primary shard都有一个或多个replica shard在别的机器上，任何一台机器宕机，都可以保证数据不会丢失，分布式搜索引擎继续可用。

### es的分布式架构原理能说一下么（es是如何实现分布式的啊）？

> **写的情况：**
>
> 1. 把索引可以拆分成多个shard（分区），每个shard存储部分数据。
> 2. 这个shard的数据实际是有多个备份，就是说每个shard都有一个primary shard（主分区），负责写入数据，但是还有几个replica shard（副本）。primary shard（主分区）写入数据之后，会将数据同步到其他几个replica shard（副本）上去。
> 3. 通过这个replica的方案，每个shard的数据都有多个备份，如果某个机器宕机了，还有别的数据副本在别的机器上。这样实现高可用。
> 4. 如果master节点宕机了，那么会重新选举一个节点为master节点。
> 5. 如果某台primary shard（主分区）宕机，那么会由master节点，让那个宕机节点上的primary shard（主分区）的身份转移到其他机器上的replica shard（副本）。
> 6. 当你修复了那台宕机的机器，重启之后，master节点会控制将缺失的replica shard分配过去，同步后续修改的数据之类的，让集群恢复正常。
>
> **读的情况：**可以从replica（副本）/primary（主分区） shard去读

### es写入数据的工作原理是什么啊？es查询数据的工作原理是什么啊？

> **写入数据：**
>
> - 写入数据的时候随便挑选一个节点，被选中的节点叫做**协调节点**，协调节点会对要写入的数据做一下Hash，然后把数据路由到某一个节点上的primary shard（主分区）上面去，路由过去了以后，就把数据同步到自己的replica shard（副本）上面，当主，副同步数据完成后，协调节点就会响应回客户端写成功了。
>
> **es搜索数据过程**
>
> 发送一个搜索请求，协调节点会把请求发送所有shard，然后所有的shard会去自己里面查，找到可能会匹配到的doc，返回到协调节点，协调节点在去doc里面去匹配，找到最符合你需要的那些document，在给你返回回来。

### 问题分析：

#### 某个 API 请求 ES 很容易造成请求超时 (抛 ConnectionTimeout)

问题排查：

造成问题的接口是给新版本 APP 里面的新功能提供的，我们发现问题时这个超时事件已经达到每天 4-6w+，虽然看起来量倒不算大，但是我依然觉得需要快速解决它:

> - 已经影响了服务质量
> - 功能比较隐蔽，如果不是主动用这个功能是不会触发 API 请求的，我体验了下对应功能确实很容易出现点开页面还没加载出来或者干脆窗口空白的情况，这太影响用户体验了
> - 这个超时量会随着新版本 APP 的装机量不断提高
> - 这些慢查询给 ES 带来压力，也影响了其他正常查询请求

这个特别慢的请求是做一个聚合计算，看一下请求的 body:

```json
{
    'aggs': {
        'by_standard_tags': {
            'terms': {
                'field': 'standard_tags.keyword',
                'size': 100
            }
        }
    },
    'query': {
        'bool': {
            'must': [{'ids': {'values': IDS}}]
        }
    }
}
```

